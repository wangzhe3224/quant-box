[{"categories":["Data Engineering"],"content":"How to schedule a computation graph (DAG) on AWS Step Functions, ie serverless","date":"2023-03-07","objectID":"/posts/serverless-9-step-functions/","tags":["aws","step-functions","stds","dag"],"title":"AWS Step Functions and DAG","uri":"/posts/serverless-9-step-functions/"},{"categories":["Data Engineering"],"content":"Oh DAG! Direct acyclic graph, DAG, is a popular data structure to organize large scale computation. because it allows you to represent the dependencies between different tasks in a clear and efficient manner. DAGs are commonly used in data processing, scientific computing, and machine learning applications to manage complex workflows. Here are some key reasons why DAGs are useful for managing computation: Clear representation of dependencies: A DAG clearly shows the dependencies between different tasks, making it easy to understand which tasks must be completed before others can begin. Efficient scheduling: With a DAG, it’s easy to schedule tasks in the most efficient way possible. By prioritizing tasks based on their dependencies, you can ensure that each task is executed only when its input data is available, which minimizes unnecessary waiting times. Parallelism: DAGs can also be used to identify opportunities for parallelism, which can significantly speed up computation. Tasks that don’t have dependencies can be executed in parallel, making better use of available resources. Incremental computation: DAGs can be used to efficiently perform incremental computations, where only the necessary parts of a computation are re-run when new data becomes available. This can save a significant amount of time and resources compared to re-running the entire computation from scratch. Overall, DAGs are a powerful tool for managing complex computations, enabling efficient scheduling, parallelism, and incremental computation, all while providing a clear representation of dependencies between tasks. As we can see, with DAG we can significantly parallelize our computations and the power of DAG let us to do fearless concurrent computation which fits in the serverless land very well. ","date":"2023-03-07","objectID":"/posts/serverless-9-step-functions/:1:0","tags":["aws","step-functions","stds","dag"],"title":"AWS Step Functions and DAG","uri":"/posts/serverless-9-step-functions/"},{"categories":["Data Engineering"],"content":"Existing DAG Based Engines There are several libraries and frameworks that use DAGs for managing computation. Here are some examples: Apache Airflow: Apache Airflow is a popular open-source platform for managing workflows and data pipelines. It uses DAGs to represent the dependencies between tasks, and provides a rich set of tools for scheduling, monitoring, and executing tasks. Dask: Dask is a Python library for parallel computing that uses DAGs to represent computation graphs. It provides a flexible framework for executing parallel and distributed computations on large datasets. TensorFlow: TensorFlow is a popular machine learning library that uses DAGs to represent computation graphs. It allows users to define complex machine learning models as a series of interconnected nodes, with each node representing a mathematical operation. Luigi: Luigi is an open-source Python package for building data pipelines. It uses DAGs to represent the dependencies between tasks, and provides a flexible framework for scheduling and executing tasks. Spark: Apache Spark is a popular distributed computing framework that uses DAGs to represent computations. It provides a powerful set of tools for parallel and distributed processing of large datasets. These libraries and frameworks are just a few examples of the many tools that use DAGs to manage computation. By using DAGs, these tools can provide a clear representation of dependencies between tasks, efficient scheduling, and parallelism, which can make it easier to manage complex workflows and large datasets. There are also some libraries, such as Incremental (JaneStreet), Man MDF (Man Group), loman, anchors, etc. More details please check: https://wangzhe3224.github.io/awesome-systematic-trading/#computation-graph ","date":"2023-03-07","objectID":"/posts/serverless-9-step-functions/:2:0","tags":["aws","step-functions","stds","dag"],"title":"AWS Step Functions and DAG","uri":"/posts/serverless-9-step-functions/"},{"categories":["Data Engineering"],"content":"Schedule The DAG Apart from describe the computation, another important aspect of using DAG for computation is the runtime or scheduling runtime. There are different ways to schedule DAG computation based on the nature of the tasks, available resources, and performance goals. Here are some examples of scheduling strategies: Serial scheduling: In this approach, tasks are executed in a sequential manner, one after the other. This is the simplest scheduling strategy and may be appropriate for small DAGs with a small number of tasks and dependencies. Parallel scheduling: Tasks are executed concurrently, taking advantage of multiple processors or threads. Parallelism can be used when tasks are independent or can be executed in parallel without interfering with each other. This can speed up computation significantly and is a common strategy for large-scale computations. Topological ordering: A topological ordering of the DAG is a linear ordering of the nodes such that if there is an edge from node A to node B, then A comes before B in the ordering. This ordering can be used to schedule tasks, with tasks at the beginning of the ordering executed first, followed by tasks later in the ordering. Dynamic scheduling: In this approach, tasks are scheduled at runtime based on the availability of data and resources. This can be useful when the execution time of tasks is uncertain or when new tasks are added to the DAG at runtime. Batch scheduling: In this approach, tasks are grouped into batches, and each batch is executed in parallel. Batching can be useful when the DAG contains a large number of tasks and parallelism can be used to speed up computation. Pipeline scheduling: In this approach, tasks are divided into stages, and each stage is executed in parallel. Each stage can have its own set of dependencies and resources, allowing for efficient use of available resources. This approach is commonly used in data processing and machine learning applications. These scheduling strategies can be combined in different ways to optimize performance and resource usage for a given DAG. The choice of scheduling strategy depends on the characteristics of the DAG and the available resources. ","date":"2023-03-07","objectID":"/posts/serverless-9-step-functions/:3:0","tags":["aws","step-functions","stds","dag"],"title":"AWS Step Functions and DAG","uri":"/posts/serverless-9-step-functions/"},{"categories":["Data Engineering"],"content":"Can DAG Go Serverless? Yes. As we discussed, there are two parts in computation with DAG: computing and scheduling. Computing is the bread and butter of serverless, we have Lambda or Fargate. However, scheduling is not that straightforward. As shown above, most of the DAG based computation engine, such as Spark, we need a long running session to coordinate the computation. This long run session need to handle the scheduling of the jobs and handling the intermediate results. Serverless is not very good at long running tasks. Luckily enough, AWS provides a workflow service: Step Functions. Step Functions is not a service designed for DAG computation, however we can make it our DAG scheduling engine with a bit efforts. ","date":"2023-03-07","objectID":"/posts/serverless-9-step-functions/:4:0","tags":["aws","step-functions","stds","dag"],"title":"AWS Step Functions and DAG","uri":"/posts/serverless-9-step-functions/"},{"categories":["Data Engineering"],"content":"Topological Order Say for example, I have following DAG: First of all, we get the topological order of the DAG as show above. With this order, the jobs are organized to several layers, for example, root node is at layer 0, xyz are in layer 1, etc. ","date":"2023-03-07","objectID":"/posts/serverless-9-step-functions/:4:1","tags":["aws","step-functions","stds","dag"],"title":"AWS Step Functions and DAG","uri":"/posts/serverless-9-step-functions/"},{"categories":["Data Engineering"],"content":"Transitive Reduction Notice that there is a dependency from x to x+(x+z) node, which is a dependency goes across two layers. This is not good! Because Step Function do not support this kind of workflow. However, if we look at it closely, this dependency is redundant in this setup, because by the time we reach x+(x+z), x is guaranteed fulfilled. This is called transitive reduction, meaning G = (V,E) is a graph G- = (V,E-) such that for all v,w in V there is an edge (v,w) in E- if and only if (v,w) is in E and there is no path from v to w in G with length greater than 1. So we can reduce the graph: This is nice and clean now and we can use Step Function to express this DAG now. ","date":"2023-03-07","objectID":"/posts/serverless-9-step-functions/:4:2","tags":["aws","step-functions","stds","dag"],"title":"AWS Step Functions and DAG","uri":"/posts/serverless-9-step-functions/"},{"categories":["Data Engineering"],"content":"Pass Parameters Transitive reduction is good, however there are some node missing input information. For example, x+(x+z) can never know it need to read x. Using Step Functions to schedule DAG is a static scheduling, meaning we know exactly the execution order, so we can encode the input and output into the config statically before running the DAG, see input and output of AWS Step Function for details.1 ","date":"2023-03-07","objectID":"/posts/serverless-9-step-functions/:4:3","tags":["aws","step-functions","stds","dag"],"title":"AWS Step Functions and DAG","uri":"/posts/serverless-9-step-functions/"},{"categories":["Data Engineering"],"content":"Intermediate Results The next piece is how can we manage the intermediate results? There could be many serverless solutions. For example, we can use S3. If we care more about the latency, we can use ElasticCache. ","date":"2023-03-07","objectID":"/posts/serverless-9-step-functions/:4:4","tags":["aws","step-functions","stds","dag"],"title":"AWS Step Functions and DAG","uri":"/posts/serverless-9-step-functions/"},{"categories":["Data Engineering"],"content":"General Lambda The last piece is what’s actually in the Lambda Invoke job in the Step Function workflow? This function should know how to load parameter from the intermediate store (previous step), then should also know which function to call to compute the result, and at last store the output to the store. Here is some demo code: store = S3Store(bucket=\"some-bucket\") path = \"store/\" def handler(event, context): \"\"\" Event example: { \"component_name\": \"HistDataLoader\", \"component_config\": { \"symbol\": \"ETHUSDT\", \"freq\": \"5m\", \"lookback\": 1, \"type\": \"future\" }, \"args\": null, \"kwargs\": null } \"\"\" component_name, config, args, output = event['component_name'], event['component_config'], event['args'], event['output'] _cls = getattr(components, component_name) obj = _cls(**config) if args is None: res = obj() else: argv = [] for arg in args: d = store.read(f\"{path}{arg}\") argv.append(d) res = obj(*argv) store.write(f\"{path}{output}\", res) return { \"output\": f\"{path}{output}\" } ","date":"2023-03-07","objectID":"/posts/serverless-9-step-functions/:4:5","tags":["aws","step-functions","stds","dag"],"title":"AWS Step Functions and DAG","uri":"/posts/serverless-9-step-functions/"},{"categories":["Data Engineering"],"content":"Potential Issues Some times, how to arrange the jobs in different layer is not straightforward. Take following graph for example: node 2x only depends on x, however, if we follow the standard compilation method, 2x can only be executed after all xyz nodes finished. Another way to schedule 2x is to put it into the same layer of xyz, so that 2x executed immediately after x is done. However, in the way, we are risking slow down the computation of the whole next layer. For example, if 2x is a very expensive computation in terms of time, it is better to schedule it in the next layer. Take it further, event 2x is scheduled in the next layer, the long computation delay problem is not resolved but just pus the delay a bit further. Fundamentally, if we cannot predict the runtime of each job, workflow approach will suffer this type of issue. In order to solve this potential issue, a dynamic scheduling is required, whereas essentially workflow is a static scheduling. I will leave that with another post. https://docs.aws.amazon.com/step-functions/latest/dg/concepts-input-output-filtering.html ↩︎ ","date":"2023-03-07","objectID":"/posts/serverless-9-step-functions/:5:0","tags":["aws","step-functions","stds","dag"],"title":"AWS Step Functions and DAG","uri":"/posts/serverless-9-step-functions/"},{"categories":["Data Engineering"],"content":"Demo how to build a data processing pipeline with serverless on aws.","date":"2023-02-17","objectID":"/posts/insider_trade_pipeline/","tags":["aws","serverless","alternative-data","stds"],"title":"Serverless Data Pipeline - SEC Insider Trades (Form 4)","uri":"/posts/insider_trade_pipeline/"},{"categories":["Data Engineering"],"content":"Introduction In this post we will demo the design of a serverless data pipeline with aws using SEC Insider Trade (Form 4) data as an example, which is public available. This post is part of the implementation of Systematic Trading Done Serverless Series. Securities and Exchange Commission, SEC, is a government agency mainly to enforce law against security market manipulation1. With Form 4 filing, the public is made aware of the insider’s various transactions in company securities, including the amount purchased or sold and the price per share. An “insider” is an officer, director, 10% stockholder and anyone who possesses inside information because of his or her relationship with the Company or with an officer, director or principal stockholder of the Company. Form 4 must be filed within two business days following the transaction date. More details check here. The form is delivered as XML file, for example a form 4 for AAPL and here is the same from in HTML format. Screenshot as below2: Basically, Form 4 discloses insider’s transactions of the company stocks or derivatives. The information we care is that: Who traded, i.e. is he/she a director or officer or 10% owner? How many shares at what price? When traded, When reported? Transaction type: directly trade? Option exercise? Buy or Sell? The goal is: give a universe of companies (identified by SEC id CIK), check if there are any new Form 4 filings uploaded, if so download the XML files and extract the insider trade information, then persist them for later usage. It would also be good that we can get some form of notification if new insider trade happened. ","date":"2023-02-17","objectID":"/posts/insider_trade_pipeline/:1:0","tags":["aws","serverless","alternative-data","stds"],"title":"Serverless Data Pipeline - SEC Insider Trades (Form 4)","uri":"/posts/insider_trade_pipeline/"},{"categories":["Data Engineering"],"content":"Design Decisions Here we break the design decision to following parts: Decision 1: How will the system scale up? Decision 2: Pick up tools from each categories of the services Decision 3: Use queue and lambda to achieve parallelization Decision 4: What kind of storage should we use to store information Decision 5: Design error path as normal path, let exception throw loudly Here is a diagram of the design in high level: High Level Design Let’s break it down. ","date":"2023-02-17","objectID":"/posts/insider_trade_pipeline/:2:0","tags":["aws","serverless","alternative-data","stds"],"title":"Serverless Data Pipeline - SEC Insider Trades (Form 4)","uri":"/posts/insider_trade_pipeline/"},{"categories":["Data Engineering"],"content":"How the Pipeline Scales Decision 1: Following id based parallelization patterns. In systematic trading world, nearly every predictor/signal starts with a data pipeline. Most of the data pipelines are name by name basis, meaning given a company or entity, there are multiple fields related to it. For example, Apple Inc., we have price data, order book data, fundamental data, alternative data such as social media, search activity, event data. As we can see there are two dimensions about the data: identifier and field. It is not entirely true, we also have a time dimension! id, N field, F time, T The next question to ask is: in which dimension we process the data? The objective function is speed, ie how can we process the data in the fastest way? I think the answer drills down to how well we parallelize our data pipeline. We can parallelize by id, by field, or event by time! Which one should we choose? It depends on which one is larger and more uniform. Time is not a good choice in my opinion as time is not even for all the ids and fields. And most of the time, the number of ids, N, is much larger than the number of fields, F. In this case, it is better to build the data pipeline by id basis, so that we gain the most ability to parallelize. In the end, the decision is we use company names as our parallelization path. ","date":"2023-02-17","objectID":"/posts/insider_trade_pipeline/:2:1","tags":["aws","serverless","alternative-data","stds"],"title":"Serverless Data Pipeline - SEC Insider Trades (Form 4)","uri":"/posts/insider_trade_pipeline/"},{"categories":["Data Engineering"],"content":"Serverless Data Pipeline Decision 2: Pick up tools from each categories of the services In the previous posts, we went through quite a few serverless tools. How to build data pipeline with to serverless tools? As I proposed before, in serverless land, we think in three components, computation, communication, adn storage: Computation Lambda, for io requests, parsing the file and extracting information Communication EventBridge, for scheduling SQS, for task management Storage S3, for file storage DynamoDB, for post-processed data query ","date":"2023-02-17","objectID":"/posts/insider_trade_pipeline/:2:2","tags":["aws","serverless","alternative-data","stds"],"title":"Serverless Data Pipeline - SEC Insider Trades (Form 4)","uri":"/posts/insider_trade_pipeline/"},{"categories":["Data Engineering"],"content":"Parallelization Decision 3: Use queue and lambda to achieve parallelization Since companies filings are independent, we can parallelize the pipeline very well using lambda and queue. This is a common design pattern in serverless land, namely queue-lambda. The idea is the have a daily scheduled job kick off to enqueue all company names, ie the universe we care. After all the jobs are in the queue, we define a lambda function to check and download filing files if any. The benefit of lambda is that we can nearly parallelize every names. The benefit of a queue-lambda structure is that we can control our parallelization via lambda’s reserved concurrency. For example, if we set the reserved concurrency to 10, we will not processing more than 10 names at the same time. There is another subtile yet interesting benefit of lambda: every code start lambda instance has a different public IP address! This is really a great feature in terms scaping type data pipeline. As we probably all know that most of the public available data source have some sort of throttling control. Say for example the SEC website has a rate limit of 10 requests per second. With the different lambda instance with different IPs, we actually increase the limit! (please… don’t do anything bad with this ….) I did some experiments to prove this, here is the calling history of a lambda function. The function is easy simply print its IP address: @2023-02-01 20:00 {\"ip\": \"18.170.55.26\"} @2023-02-01 23:30 {\"ip\": \"18.169.190.4\"} @2023-02-01 23:48 {\"ip\": \"18.134.99.111\"} @2023-02-02 11:17 {\"ip\": \"3.8.162.74\"} @2023-02-02 20:39 {\"ip\": \"18.168.205.156\"} @2023-02-02 22:04 {\"ip\": \"13.42.48.110\"} It turns out, we can get quite a few of public IP address for free with lambda. ","date":"2023-02-17","objectID":"/posts/insider_trade_pipeline/:2:3","tags":["aws","serverless","alternative-data","stds"],"title":"Serverless Data Pipeline - SEC Insider Trades (Form 4)","uri":"/posts/insider_trade_pipeline/"},{"categories":["Data Engineering"],"content":"SQL or NoSQL or S3 Decision 4: What kind of storage should we use to store information In this pipeline, there are two pieces of information I would like to keep: the raw Form 4 and processed Form 4. Raw data is just the file we download from SEC website. The processed data is bit more interesting. I only picked following fields: res = { \"owner\": owner, \"cik\": cik, \"name\": name, \"symbol\": symbol, \"tx_date\": tx_date, \"tx_code\": tx_code, \"tx_side\": tx_side, \"tx_shares\": tx_shares, \"post_tx_amount\": post_tx_amount, \"access_number\": access_number, \"filing_date\": filing_date, } As long as we persist the raw data files, it is always possible to add another downstream jobs to produced another set of post-process information. Next question is: where shall we store the data? Raw Data For the raw data, it is obvious that S3 is the place to go! The interesting part is how can we design the folder and file name. I proposed following: {BUCKET}/{CIK}/{From}/{FilingDate}#{AccessNumber}.txt. Although this pipeline only focus on Form 4, I still add a sub path {From} to identify potential more filing forms in the future. Processed Data Processed data is almost like a dict in Python, it is highly structured (more structural then XML? Not sure.). And it is a timeseries data by nature. We have few options here: S3, DynamoDB (No-SQL), and SQL. S3 seems not very good as lack of query abilities out of box. We could use is as a underlying file system and build a query layer on top, but it sounds to much and we have better options. I indeed hesitate about DynamoBD and SQL (Serverless version AWS RDS - A relational database). For DynamoDB is THE goto serverless database on aws, it is highly scalable and easy to setup. The tricky bit is the primary key and secondary index design, see details. On the other hand, RDS a SQL database can never go wrong, right?? Tables, Primary key, foreign key, on top all of these, we have powerful query planner, we can do all sort of join, aggregation on the fly. Let’s have a look at our data again. It is simple, a key value pair. Let’s say we have 2000 names, 1 files per day. The amount of the data is still very small. Both DynamoDB and RDS fit our purpose. How to select? The answer is a bit surprise: cost. With our data, DynamoDB is FREE to use. So the decision is DynamoDB! ","date":"2023-02-17","objectID":"/posts/insider_trade_pipeline/:2:4","tags":["aws","serverless","alternative-data","stds"],"title":"Serverless Data Pipeline - SEC Insider Trades (Form 4)","uri":"/posts/insider_trade_pipeline/"},{"categories":["Data Engineering"],"content":"Error Handling Decision 5: Design error path as normal path, let exception throw loudly In our data pipeline there are two potential failure points: Filing download job Filing processing job The download job is a web io job, it could fail for all sort of reasons, limit cap, network error, etc. The processing job could fail as well, for example, a change XML structure could just fail our parser. We have to things to handle failures: retry and dead-letter queue. We could config lambda function to retry 2 time, after all of them failed, the job will be pushed into a dead-letter queue, which will trigger downstream notification to alert the failures. ","date":"2023-02-17","objectID":"/posts/insider_trade_pipeline/:2:5","tags":["aws","serverless","alternative-data","stds"],"title":"Serverless Data Pipeline - SEC Insider Trades (Form 4)","uri":"/posts/insider_trade_pipeline/"},{"categories":["Data Engineering"],"content":"Notification Notification on aws is SNS. There are two types of notifications: on purpose and on failures. In this design, we use DynamoDB stream to publish topics to SNS for on purpose notification. Whereas, on failures notification is done with a lambda consume dead-letter queue items, then publish to a failure topic in SNS. ","date":"2023-02-17","objectID":"/posts/insider_trade_pipeline/:2:6","tags":["aws","serverless","alternative-data","stds"],"title":"Serverless Data Pipeline - SEC Insider Trades (Form 4)","uri":"/posts/insider_trade_pipeline/"},{"categories":["Data Engineering"],"content":"Implementation Ah wow, it takes some time to reach this point: implementation! Given this post becomes too long to read (I am trying to limit each post under 2000 words), I will leave the details with next post. Some key points to encourage you keep an eye: How to develop serverless data pipeline with AWS SAM? How to design the dynamoDB primary key for processed data? How to set batch behavior for queue and lambda? How S3 can trigger a lambda worker? How to handle reentry of the lambda function? How to avoid duplicated jobs? (as we know standard SQS is at least once delivery not exact once delivery) I am also working to release the source code for the implementation as well. ","date":"2023-02-17","objectID":"/posts/insider_trade_pipeline/:3:0","tags":["aws","serverless","alternative-data","stds"],"title":"Serverless Data Pipeline - SEC Insider Trades (Form 4)","uri":"/posts/insider_trade_pipeline/"},{"categories":["Data Engineering"],"content":"Summary In this post, we went through the design process of a serverless data pipeline. The example dataset is SEC Insider Trader filing form 4. We discussed varies design decisions, such as scalability, parallelization, storage, and error handling. Interestingly, some time cost is also a design perspective in serverless. For example, for this design, if we run about 505 names, it is almost free! After the SNS topic of filing topic update, we could develop other downstream jobs, for example computing a signal based on insider trade event. Or some aggregation job to transform the post-processed results to other easy to use format. https://www.investopedia.com/terms/s/sec.asp ↩︎ https://www.sec.gov/edgar/browse/?CIK=320193\u0026owner=exclude ↩︎ ","date":"2023-02-17","objectID":"/posts/insider_trade_pipeline/:4:0","tags":["aws","serverless","alternative-data","stds"],"title":"Serverless Data Pipeline - SEC Insider Trades (Form 4)","uri":"/posts/insider_trade_pipeline/"},{"categories":["Software Design"],"content":"(STDS) 7 - DynamoDB","date":"2023-02-13","objectID":"/posts/serverless-7-dynamodb/","tags":["serverless","aws","systematic-trading","stds","dynamodb"],"title":"Systematic Trading Done Serverless (7) - DynamoDB","uri":"/posts/serverless-7-dynamodb/"},{"categories":["Software Design"],"content":"What is DynamoDB DynamoDB is THE serverless storage solution on aws, which is a fully managed, serverless, key-value NoSQL database designed to run high-performance applications at any scale. It is: Distributed, meaning if we have tables across multi region, we need to think about consistency good horizontal scaling ability NoSQL, meaning Better for fix pattern query instead of ad-hoc query Schemaless Keys and index are important Primary Key Partition Key Sort Key Secondary index The purpose of DynamoDB is that no matter the size of your database or the number of concurrent queries, DynamoDB aims to provide the same single-digit millisecond response time for all operations. ","date":"2023-02-13","objectID":"/posts/serverless-7-dynamodb/:1:0","tags":["serverless","aws","systematic-trading","stds","dynamodb"],"title":"Systematic Trading Done Serverless (7) - DynamoDB","uri":"/posts/serverless-7-dynamodb/"},{"categories":["Software Design"],"content":"Core Concept of DynamoDB The basic concept of DynamoDB are: Tables. A table is a collection of data (items) table name primary key, to uniquely identify an item single partition key composite key: partition key + sort key Items. Each table contains 0 or more items. An item is a group of attributes that is uniquely identifiable among all the other items. Attributes. Each item is composed of 1 or more attribute, which is fundamental, and cannot be broken down anymore. ","date":"2023-02-13","objectID":"/posts/serverless-7-dynamodb/:2:0","tags":["serverless","aws","systematic-trading","stds","dynamodb"],"title":"Systematic Trading Done Serverless (7) - DynamoDB","uri":"/posts/serverless-7-dynamodb/"},{"categories":["Software Design"],"content":"Primary Key The partition key (PK) also called a hash key, as the name suggested, PK works like a key in a dictionary, by which we can find the item directly. The PK determines where our item are stored. With the sort key (SK), we can carry out more flexible query operation instead of just read PK. Essentially, with SK and PK, we can model a 1 to many relationship, meaning several items can have same PK (so stored together) as long as they have different SK assigned. ","date":"2023-02-13","objectID":"/posts/serverless-7-dynamodb/:2:1","tags":["serverless","aws","systematic-trading","stds","dynamodb"],"title":"Systematic Trading Done Serverless (7) - DynamoDB","uri":"/posts/serverless-7-dynamodb/"},{"categories":["Software Design"],"content":"Secondary Index Apart from using PK and SK to query/fetch data, Secondary Index is another option to query the table using an alternative key. In concept, secondary index creates a new “primary key” (either single or composite) for the table: global secondary index can have different PK and SK than the original primary key local secondary index can only have different SK thant the original primary key Note that we secondary index created in the table, every time the table changed, either add, update, or delete, the index will be updated automatically. This implies additional performance cost and of course $ cost. In exchange, we get flexible query pattern and much better query performance. ","date":"2023-02-13","objectID":"/posts/serverless-7-dynamodb/:2:2","tags":["serverless","aws","systematic-trading","stds","dynamodb"],"title":"Systematic Trading Done Serverless (7) - DynamoDB","uri":"/posts/serverless-7-dynamodb/"},{"categories":["Software Design"],"content":"DynamoDB Stream We mentioned DynamoDB stream in the post about triggers of lambda. It is an optional feature that capture data modification events in DynamoDB tables. This could be very useful to build event driven applications. Each event is represented by a stream record. If you enable a stream on a table, DynamoDB Streams writes a stream record whenever one of the following events occurs: A new item is added to the table: The stream captures an image of the entire item, including all of its attributes. An item is updated: The stream captures the “before” and “after” image of any attributes that were modified in the item. An item is deleted from the table: The stream captures an image of the entire item before it was deleted. Each stream record also contains the name of the table, the event timestamp, and other metadata. Stream records have a lifetime of 24 hours; after that, they are automatically removed from the stream. ","date":"2023-02-13","objectID":"/posts/serverless-7-dynamodb/:2:3","tags":["serverless","aws","systematic-trading","stds","dynamodb"],"title":"Systematic Trading Done Serverless (7) - DynamoDB","uri":"/posts/serverless-7-dynamodb/"},{"categories":["Software Design"],"content":"Read/Write Capacity Similar to other serverless service, DynamoDB has to mode to control scaling behavior (throughput and capacity): On-demand Provision ( Default )1 I quote from aws documentation2: Amazon DynamoDB on-demand is a flexible billing option capable of serving thousands of requests per second without capacity planning. DynamoDB on-demand offers pay-per-request pricing for read and write requests so that you pay only for what you use. On-demand mode is a good option if any of the following are true: You create new tables with unknown workloads. You have unpredictable application traffic. You prefer the ease of paying for only what you use. If you choose provisioned mode, you specify the number of reads and writes per second that you require for your application. You can use auto scaling to adjust your table’s provisioned capacity automatically in response to traffic changes. This helps you govern your DynamoDB use to stay at or below a defined request rate in order to obtain cost predictability. Provisioned mode is a good option if any of the following are true: You have predictable application traffic. You run applications whose traffic is consistent or ramps gradually. You can forecast capacity requirements to control costs. Here is a concise video introduction to core concepts of DynamoDB: https://www.youtube.com/watch?v=Mw8wCj0gkRc\u0026t=231s ","date":"2023-02-13","objectID":"/posts/serverless-7-dynamodb/:2:4","tags":["serverless","aws","systematic-trading","stds","dynamodb"],"title":"Systematic Trading Done Serverless (7) - DynamoDB","uri":"/posts/serverless-7-dynamodb/"},{"categories":["Software Design"],"content":"Setup DynamoDB Table Note that we can test dynamodb locally instead of on cloud. 3 The most basic table setup can be expressed as: { TableName : \"Music\", // Note: DynamoDB is schemaless except for the keys! We need to define AttributeDefinitions for all the keys KeySchema: [ { AttributeName: \"Artist\", KeyType: \"HASH\", //Partition key }, { AttributeName: \"SongTitle\", KeyType: \"RANGE\" //Sort key } ], // An array of attributes that describe the key schema for the table and indexes. // Note: Do NOT including anything thing other than keys AttributeDefinitions: [ { AttributeName: \"Artist\", AttributeType: \"S\" }, { AttributeName: \"SongTitle\", AttributeType: \"S\" }, ], ProvisionedThroughput: { // Only specified if using provisioned mode ReadCapacityUnits: 1, WriteCapacityUnits: 1 } } ","date":"2023-02-13","objectID":"/posts/serverless-7-dynamodb/:3:0","tags":["serverless","aws","systematic-trading","stds","dynamodb"],"title":"Systematic Trading Done Serverless (7) - DynamoDB","uri":"/posts/serverless-7-dynamodb/"},{"categories":["Software Design"],"content":"DynamoDB Mindset DynamoDB’s API is somehow different from other database, especially SQL database. DynamoDB does not have a build-in query planer or other optimization under the hood. It exposes a direct access the raw data. The only tooling we have to query is primary key ( PK and SK ) plus other secondary indices, which essentially another set of primary keys. This mapped directly to the underlying data structure of dynamodb: partitions (PK) and B-Tree (SK). There is no join or aggregation operation in DynamoDB. The benefit is scalability. This direct API mindset affects the way we design DynamoDB tables and sometimes makes me feel unconformable as I am used to SQL databases where I can join, aggragate on the fly. You can always get back to a full scan of the table to do any kind of query, although this is the last solution you would like to go to. It is always better to understand the use case of the table and design the PK, SK, and secondary indices properly to avoid scanning, which can be super slow if the table grows. Here is a very good article discussing the designing of DynamoDB table4. The rule of thumb is that: any thing looks like a SQL table may be a bad design for DynamoDB table. DynamoDB encourages to put related data together, not only logically but physically with PK, and do less query but fetch all relevant data back at one time. There is a nice book discussing the DynamoDB design in details: https://www.dynamodbbook.com/ A more concise design process provided by aws: https://docs.aws.amazon.com/prescriptive-guidance/latest/dynamodb-data-modeling/step3.html DynamoDB provides 200 million read/write requests per month and 25GB space for FREE. ↩︎ https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html#HowItWorks.OnDemand ↩︎ https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DynamoDBLocal.DownloadingAndRunning.html ↩︎ https://aws.amazon.com/blogs/database/single-table-vs-multi-table-design-in-amazon-dynamodb/ ↩︎ ","date":"2023-02-13","objectID":"/posts/serverless-7-dynamodb/:4:0","tags":["serverless","aws","systematic-trading","stds","dynamodb"],"title":"Systematic Trading Done Serverless (7) - DynamoDB","uri":"/posts/serverless-7-dynamodb/"},{"categories":["Software Design"],"content":"STDS X1 - Lambda is powerful, triggers are the handler to the power","date":"2023-02-10","objectID":"/posts/serverless-x1-lambda-trigger/","tags":["lambda","serverless","aws","stds"],"title":"STDS x1 - Triggers of AWS Lambda","uri":"/posts/serverless-x1-lambda-trigger/"},{"categories":["Software Design"],"content":"lambda is a piece of code ( or a function ) that is ready to run. However who triggers them? how to pass parameters to them? This is the topic of this post. ","date":"2023-02-10","objectID":"/posts/serverless-x1-lambda-trigger/:0:0","tags":["lambda","serverless","aws","stds"],"title":"STDS x1 - Triggers of AWS Lambda","uri":"/posts/serverless-x1-lambda-trigger/"},{"categories":["Software Design"],"content":"Triggers There are few ways to trigger lambda in aws: API Gateway / Lambda URL Time based scheduler ( Implemented by EventBridge ) Event Source Mapping Queue or Message Bus SQS SNS EventBridge Kinesis Kafka ( self managed or aws managed ) Other services DynamoDB S3 and almost every other aws services can trigger lambda Step Functions Those triggers are very rich, up on which we can make the whole aws infrastructure becomes our event driven architecture. The event triggers are bridges from serverless components to server based component, which makes the system so plugable. ","date":"2023-02-10","objectID":"/posts/serverless-x1-lambda-trigger/:1:0","tags":["lambda","serverless","aws","stds"],"title":"STDS x1 - Triggers of AWS Lambda","uri":"/posts/serverless-x1-lambda-trigger/"},{"categories":["Software Design"],"content":"API Gateway and Lambda URL API Gateway is probably the most straightforward way to invoke a lambda function. API Gateway and Lambda User or other processes can simple by calling the REST API to pass payload to the lambda function and expect a result. The calling can either be sync or async depending on the actual lambda function implementation. However, API Gateway is much more powerful ( for good and for bad ), not only we can send HTTP request, but also Websocket! In addition, it come with more stuff such as API key, caching, CORS, throttling, etc. If you just want build a internal REST API, API Gateway probably a bit too much. The other option is Lambda URL, which is a new service released 2022. With Lambda URL, we will get a IAM auth based http endpoint for the lambda function. We cannot setup a customized domain for it, neither caching, but it is cheaper as well than API Gateway. ","date":"2023-02-10","objectID":"/posts/serverless-x1-lambda-trigger/:2:0","tags":["lambda","serverless","aws","stds"],"title":"STDS x1 - Triggers of AWS Lambda","uri":"/posts/serverless-x1-lambda-trigger/"},{"categories":["Software Design"],"content":"Time Scheduler This is basically put a crontab job for lambdas. For example, you can setup a daily job to run every weekday at 17:00. Two things to mention: scheduler is actually based on EventBridge, which we will cover later. there is no user defined information passed to lambda function. ( lambda function always receive some system status information, by convention they are encapsulated in Event and Context objects. ) ","date":"2023-02-10","objectID":"/posts/serverless-x1-lambda-trigger/:3:0","tags":["lambda","serverless","aws","stds"],"title":"STDS x1 - Triggers of AWS Lambda","uri":"/posts/serverless-x1-lambda-trigger/"},{"categories":["Software Design"],"content":"Event Source Mapping ","date":"2023-02-10","objectID":"/posts/serverless-x1-lambda-trigger/:4:0","tags":["lambda","serverless","aws","stds"],"title":"STDS x1 - Triggers of AWS Lambda","uri":"/posts/serverless-x1-lambda-trigger/"},{"categories":["Software Design"],"content":"SQS SQS is a queue service. We can setup lambda functions to be triggered by the message in the queue. Technically, lambda is not triggered by the message in the queue, whereas it is aws will let the lambda polls the queue for us, if it find any message, the lambda will process the message. SQS will send message in batch to lambda, once sent to lambda, that batch will be marked with invisible, if the lambda successfully processed the message, those messages are deleted, otherwise, failed batch (note that whole batch is back!) will be put back to the queue. After a predefined retry reached, failed message will be put into a dead letter queue. ","date":"2023-02-10","objectID":"/posts/serverless-x1-lambda-trigger/:4:1","tags":["lambda","serverless","aws","stds"],"title":"STDS x1 - Triggers of AWS Lambda","uri":"/posts/serverless-x1-lambda-trigger/"},{"categories":["Software Design"],"content":"SNS SNS, simple notification service, looks similar to SQS at first glance. However, they are different by nature. SNS is a event bus or event broker, it bring messages under a topic to the topics subscribers. Plus, SNS has a much rich message filter functionality. Most importantly, SNS won’t persist any message. To use SNS, we need to register lambda functions to topics of SNS. Then if there is a message in the topic, the lambda function will be trigger with the message. ","date":"2023-02-10","objectID":"/posts/serverless-x1-lambda-trigger/:4:2","tags":["lambda","serverless","aws","stds"],"title":"STDS x1 - Triggers of AWS Lambda","uri":"/posts/serverless-x1-lambda-trigger/"},{"categories":["Software Design"],"content":"EventBridge, Kinesis, Kafka Those services are event bus as well just like SNS, however, each of them a slightly different purposes, hence different functionality and performance profiles. I think the details worth another post. For this post, we just need know: like SNS, messages are sent to lambda ( no polling ) ","date":"2023-02-10","objectID":"/posts/serverless-x1-lambda-trigger/:4:3","tags":["lambda","serverless","aws","stds"],"title":"STDS x1 - Triggers of AWS Lambda","uri":"/posts/serverless-x1-lambda-trigger/"},{"categories":["Software Design"],"content":"DynamoDB DynamoDB is AWS’s answer to NoSQL. The trigger is called DynamoDB Stream. Every time the a mutable change in the dynamodb table happens, a event is pushed into the stream. We can setup lambda function to poll the stream 4 time per second. In this way, a mutable change in the table can trigger lambda functions. We can only setup 2 lambda functions to 1 stream. ","date":"2023-02-10","objectID":"/posts/serverless-x1-lambda-trigger/:4:4","tags":["lambda","serverless","aws","stds"],"title":"STDS x1 - Triggers of AWS Lambda","uri":"/posts/serverless-x1-lambda-trigger/"},{"categories":["Software Design"],"content":"S3 S3 is AWS’s object store. There are two ways S3 trigger lambda: Event Notification, which is similar to dynamodb stream S3 Object lambda Object lambda is somehow unique, as you can add your own code to Amazon S3 GET, HEAD, and LIST requests to modify and process data before it is returned to an application. ","date":"2023-02-10","objectID":"/posts/serverless-x1-lambda-trigger/:4:5","tags":["lambda","serverless","aws","stds"],"title":"STDS x1 - Triggers of AWS Lambda","uri":"/posts/serverless-x1-lambda-trigger/"},{"categories":["Software Design"],"content":"Step Functions Technically speaking, Step Functions is an orchestration tool to connect pretty much every services in aws including lambda. With Step Function, not only we can chain lambda with events, but also we can chain lambdas with lambdas. In step function workflow, we can connect dynamodb, sns, sqs, lambda directly without boilerplate code. I won’t expand the details in this post as it is too big as a topic. For this post, it is good enough we know that we can use step functions as a tool to trigger lambda functions! ","date":"2023-02-10","objectID":"/posts/serverless-x1-lambda-trigger/:5:0","tags":["lambda","serverless","aws","stds"],"title":"STDS x1 - Triggers of AWS Lambda","uri":"/posts/serverless-x1-lambda-trigger/"},{"categories":["Software Design"],"content":"Summary Understand triggers of lambda is very important to leverage lambda’s power. Those triggers actually changes the way we design our backend software and workflows Most of the event/message based triggers have batching behavior, so we need to implement the batch handling logic ourselves, including batch window and batch size. Error and failure handling of each trigger is slightly as well. Step functions is a powerful tooling for orchestration aws services. ","date":"2023-02-10","objectID":"/posts/serverless-x1-lambda-trigger/:6:0","tags":["lambda","serverless","aws","stds"],"title":"STDS x1 - Triggers of AWS Lambda","uri":"/posts/serverless-x1-lambda-trigger/"},{"categories":["Software Design"],"content":"(STDS) Part 4 - Simple Queue Service (SQS)","date":"2023-02-08","objectID":"/posts/serverless-4-queue/","tags":["serverless","aws","sqs","stds"],"title":"Systematic Trading Done Serverless (4) - Queue","uri":"/posts/serverless-4-queue/"},{"categories":["Software Design"],"content":" Some of the content in this post is written by ChatGPT3. Guess which parts? :) In the previous posts, we’ve met two computation tools in serverless: lambda and fargate. From this post, we will explore the tools that integrate computation together. Today’s topic is queue. ","date":"2023-02-08","objectID":"/posts/serverless-4-queue/:0:0","tags":["serverless","aws","sqs","stds"],"title":"Systematic Trading Done Serverless (4) - Queue","uri":"/posts/serverless-4-queue/"},{"categories":["Software Design"],"content":"Quick Review of Queue Queue is a simple yet powerful data structure. There are so many type of queues, such as First-in-First-out (FIFO), priority queue, unordered queue, bounded, unbounded, circular, etc. Some of the important uses of queues include: Task scheduling: Queues are used to schedule tasks in a computer system, such as printing, background processing, and other types of batch processing. Resource allocation: Queues can be used to manage resource allocation, such as managing requests for disk I/O or network access. Event management: Queues can be used to manage events that need to be processed in a specific order, such as user interactions or network packets. Load balancing: Queues can be used to distribute workloads across multiple processing units, helping to ensure that resources are used efficiently and avoid overloading individual processors. Overall, queues are a useful data structure for managing processes and resources in computer systems, and they help to ensure that tasks are performed in the desired order and with the desired level of efficiency. ","date":"2023-02-08","objectID":"/posts/serverless-4-queue/:1:0","tags":["serverless","aws","sqs","stds"],"title":"Systematic Trading Done Serverless (4) - Queue","uri":"/posts/serverless-4-queue/"},{"categories":["Software Design"],"content":"SQS - Serverless Queue In the previous section, we talked more about the abstraction of queue, now let’s zoom in to AWS serverless and have a look at the oldest serverless service in AWS - Simple Queue Service (SQS). Fun Fact: AWS always use first letters as the abbreviate of the service with number for duplication. For example, EC2 - Elastic Compute Cloud, S3 - Simple Storage Service Amazon Simple Queue Service (SQS) is a fully managed message queue service provided by Amazon Web Services (AWS). It provides a simple way to decouple and scale microservices, distributed systems, and serverless applications. With SQS, you can send, store, and receive messages between software components at any scale, without losing messages or requiring other services to be available. The service enables you to transmit any volume of data, at any level of throughput, without losing messages or requiring other services to be available. Here is the important part, there are two type queues SQS provides: FIFO and Standard. FIFO exact once delivery ordered lower throughput higher latency1 Standard at least once delivery best effort ordering higher throughput lower latency1 ","date":"2023-02-08","objectID":"/posts/serverless-4-queue/:2:0","tags":["serverless","aws","sqs","stds"],"title":"Systematic Trading Done Serverless (4) - Queue","uri":"/posts/serverless-4-queue/"},{"categories":["Software Design"],"content":"Queue vs Message Broker It would be helpful here to discuss an very close related topic to queue: message broker. These two words have some overlap in my mind. If I ask: is Kafka2 a message queue or a message broker? It is a bit of both. When it comes to AWS, we immediately think of SNS (Simple Notification System) and EventBridge. SNS and EventBridge are more like a message broker rather than a queue. (hence they are separate services, although SNS is very similar to EventBridge) I have to say AWS some time is very confusing.. SQS, SNS, EventBridge, Kinesis? I as asking why can’t we just have a aws “kafka”? And guess what, I found a aws managed kafka cluster service… which make things even more complicated… So here I give my rule to differentiate queue and broker: Broker more about sub/pub pattern usually don’t persist message ( not true, at least we don’t expect it to persist ) has topics Queue more about communication and polling usually persist message for later consumption has message types ","date":"2023-02-08","objectID":"/posts/serverless-4-queue/:3:0","tags":["serverless","aws","sqs","stds"],"title":"Systematic Trading Done Serverless (4) - Queue","uri":"/posts/serverless-4-queue/"},{"categories":["Software Design"],"content":"Use Cases SQS is very useful when it comes to serverless design: Decoupling Microservices: SQS can be used to decouple microservices, allowing different parts of a system to run independently and asynchronously. This helps to ensure that one part of the system can continue to operate even if another part fails. Serverless Applications: SQS can be used as the backbone for serverless architectures, providing a simple and scalable way to transmit messages between serverless functions. Distributed Systems: SQS can be used to build and operate distributed systems, providing a way to transmit messages between multiple systems and components. Background Jobs: SQS can be used to manage background jobs, such as image processing, data analysis, and other long-running tasks. Workflow Processing: SQS can be used to manage workflows, providing a way to transmit messages between multiple steps in a workflow. Event-Driven Architecture: SQS can be used as the backbone for event-driven architectures, providing a way to transmit events between components and trigger subsequent actions. (^^ a very typical ChatGPT answer: right but somehow useless… ) Let’s see if I can do better thant ChatGPT! ","date":"2023-02-08","objectID":"/posts/serverless-4-queue/:4:0","tags":["serverless","aws","sqs","stds"],"title":"Systematic Trading Done Serverless (4) - Queue","uri":"/posts/serverless-4-queue/"},{"categories":["Software Design"],"content":"Priority Queue We can use two separate SQS queue to construct a priority queue pattern together with lambda. Basically, we can set two event triggers lambda function, q1 and 12. Then in the lambda handler, we build following logic: Poll q1, the priority queue, if has item, process it if not, poll q2 Priority Queue In this way, we can push important item into priority queue q1 and less important items to q2. ","date":"2023-02-08","objectID":"/posts/serverless-4-queue/:4:1","tags":["serverless","aws","sqs","stds"],"title":"Systematic Trading Done Serverless (4) - Queue","uri":"/posts/serverless-4-queue/"},{"categories":["Software Design"],"content":"Async Task Processing This is a typical queue use case. Say we have some function to do backtesting given some parameters. The backtesting takes very long time to finish. To avoid timeout of the API call, we could connect job lambda function to a working queue and polling for the next job to do if any. A sample design could be like this: Async Task Queue ","date":"2023-02-08","objectID":"/posts/serverless-4-queue/:4:2","tags":["serverless","aws","sqs","stds"],"title":"Systematic Trading Done Serverless (4) - Queue","uri":"/posts/serverless-4-queue/"},{"categories":["Software Design"],"content":"Auto-Scaling Give a job queue with a lot of jobs to do, we could auto scaling our workers number based on how much jobs in the queue!3 https://bitesizedserverless.com/bite/serverless-messaging-latency-compared/ ↩︎ ↩︎ https://kafka.apache.org/ ↩︎ https://jayendrapatil.com/aws-sqs-simple-queue-service/ ↩︎ ","date":"2023-02-08","objectID":"/posts/serverless-4-queue/:4:3","tags":["serverless","aws","sqs","stds"],"title":"Systematic Trading Done Serverless (4) - Queue","uri":"/posts/serverless-4-queue/"},{"categories":["Software Design"],"content":"(STDS) Part 3 - Compute with Fargate. What is Fargate? Why using Fargate? Compare with Lambda?","date":"2023-02-07","objectID":"/posts/serverless-3-fargate/","tags":["serverless","aws","fargate","stds"],"title":"Systematic Trading Done Serverless (3) - Fargate","uri":"/posts/serverless-3-fargate/"},{"categories":["Software Design"],"content":"What is Fargate AWS fargate is serverless compute for containers. In concept, fargate provide a service that user only need to provide a image containing business logic, and AWS will handle the container management. If we put the other serverless computation engine lambda with fargate1: EC2 vs Lambda vs Fargate We can see that fargate is just between a server solution (ec2) and lambda in terms of flexibility and operation burden. As this series is about serverless let’s compare lambda and fargate, which are the only computation engine in our serverless toolbox. (This is not to say we cannot use server based services in a serverless architecture, in fact, once we follow serverless design patterns ie micro-service and event based, it is very easy to plug any server based services.) ","date":"2023-02-07","objectID":"/posts/serverless-3-fargate/:1:0","tags":["serverless","aws","fargate","stds"],"title":"Systematic Trading Done Serverless (3) - Fargate","uri":"/posts/serverless-3-fargate/"},{"categories":["Software Design"],"content":"Lambda vs Fargate ","date":"2023-02-07","objectID":"/posts/serverless-3-fargate/:2:0","tags":["serverless","aws","fargate","stds"],"title":"Systematic Trading Done Serverless (3) - Fargate","uri":"/posts/serverless-3-fargate/"},{"categories":["Software Design"],"content":"Start Time Lambda job starts much faster than Fargate tasks. In lambda even the slow cold start is counting in milliseconds, whereas with Fargate, we are talking about minutes. The long start time makes fargate more suitable for long lasting sessions so that the cost of startup is meaningful, whereas lambda is for those jobs short lived. ","date":"2023-02-07","objectID":"/posts/serverless-3-fargate/:2:1","tags":["serverless","aws","fargate","stds"],"title":"Systematic Trading Done Serverless (3) - Fargate","uri":"/posts/serverless-3-fargate/"},{"categories":["Software Design"],"content":"Computation Power In terms of computation power, fargate is much more powerful than lambda (with trade off the start time). Fargate Configs With fargate, you can get a container with 16 vCores and 120 GB memory to run your image, whereas in lambda the memory is capped with 10 GB (as of 2023, aws seems increasing the limit during the past years). And in lambda the CPU is allocated proportion to memory sizing with the cap of 6 vCores. In systematic trading land, there are definitely some tasks that requires a beefy container to run. ","date":"2023-02-07","objectID":"/posts/serverless-3-fargate/:2:2","tags":["serverless","aws","fargate","stds"],"title":"Systematic Trading Done Serverless (3) - Fargate","uri":"/posts/serverless-3-fargate/"},{"categories":["Software Design"],"content":"Duration Fargate jobs can run forever just like a normal server, whereas lambda has a duration cap at 15 min. Haven said that, if you have a task that need to run 247, probably EC2 is what you need, because running same size fargate container 247 is more expensive that a same size EC2 box. In systematic trading land, if you have some latency sensitive task that need a much longer session, say during US market open hours, fargate is the best choice. ","date":"2023-02-07","objectID":"/posts/serverless-3-fargate/:2:3","tags":["serverless","aws","fargate","stds"],"title":"Systematic Trading Done Serverless (3) - Fargate","uri":"/posts/serverless-3-fargate/"},{"categories":["Software Design"],"content":"Operation and Scaling Fargate is more involved in terms of operation, for example, the auto-scaling of fargate is not as straightforward as lambda. With lambda there is very little operation to do and auto scaling is really, say, auto. On the other side of the coin, more operation means more control. Fargate proviode more control to the runtime. ","date":"2023-02-07","objectID":"/posts/serverless-3-fargate/:2:4","tags":["serverless","aws","fargate","stds"],"title":"Systematic Trading Done Serverless (3) - Fargate","uri":"/posts/serverless-3-fargate/"},{"categories":["Software Design"],"content":"When to choose Fargate Choose Fargate: you want to migrate your docker container service to serverless any job you want to run more than 15 min long session jobs low latency jobs computation intensive jobs, for example requirement more than 10 GB memory Choose Lambda: short session jobs latency is not a big deal computation less intensive Although before deicide lambda and fargate, it is always better to think about the structure of the task. Can we simply break down a big task that need big fargate container to several lambda functions? From my experience, in some cases use Step Function with lambda can simple break down the big fargate jobs. We also need to think about the cost. The worst thing we can do is that we use fargate and it turns out it is more expensive than a EC2 solution. ","date":"2023-02-07","objectID":"/posts/serverless-3-fargate/:3:0","tags":["serverless","aws","fargate","stds"],"title":"Systematic Trading Done Serverless (3) - Fargate","uri":"/posts/serverless-3-fargate/"},{"categories":["Software Design"],"content":"Final Thought Comparing to Lambda, Fargate is more like traditional server host solution. It does not affect design architecture as much as lambda does. However, we should never stick to a toolbox and ignore all other tools, in the end our target is to build a software, trading system in our case, we should use more tooling to help us as long as it fit in the system. ","date":"2023-02-07","objectID":"/posts/serverless-3-fargate/:4:0","tags":["serverless","aws","fargate","stds"],"title":"Systematic Trading Done Serverless (3) - Fargate","uri":"/posts/serverless-3-fargate/"},{"categories":["Software Design"],"content":"References Introducing AWS Fargate - AWS Online Tech Talks https://medium.com/thundra/getting-it-right-between-ec2-fargate-and-lambda-bb42220b8c79 ↩︎ ","date":"2023-02-07","objectID":"/posts/serverless-3-fargate/:5:0","tags":["serverless","aws","fargate","stds"],"title":"Systematic Trading Done Serverless (3) - Fargate","uri":"/posts/serverless-3-fargate/"},{"categories":["Software Design"],"content":"(STDS) Part 2 - Compute with Lambda. What is lambda? Why using lambda? How to use lambda? and downside of lambda.","date":"2023-02-04","objectID":"/posts/serverless-2-lambda/","tags":["serverless","aws","lambda","systematic-trading","stds"],"title":"Systematic Trading Done Serverless (2) - Lambda","uri":"/posts/serverless-2-lambda/"},{"categories":["Software Design"],"content":"Lambda is probably the most fundamental part of serverless architecture. Lambda is NOT silver bullet. ","date":"2023-02-04","objectID":"/posts/serverless-2-lambda/:0:0","tags":["serverless","aws","lambda","systematic-trading","stds"],"title":"Systematic Trading Done Serverless (2) - Lambda","uri":"/posts/serverless-2-lambda/"},{"categories":["Software Design"],"content":"What is Lambda Here is a “official” definition from AWS: AWS Lambda is a compute service that lets you run code without provisioning or managing servers. Lambda runs your code on a high-availability compute infrastructure and performs all of the administration of the compute resources, including server and operating system maintenance, capacity provisioning and automatic scaling, and logging. Ok. Let’s go unofficial. It feels like aws lambda has a connections to Lambda Calculus, where lambda is a symbol denoting the start of a function definition. And I think it is very helpful to think lambda as a function. lambda service is basically a runtime that gives you all the CPU/memory to run function you defined (code or binary). It is a natural evolution of the virtualization of the computing. As shown below1. Lambda removes the host/server from developer’s mindset. Where Lambda come from? I view lambda as a box with all resources (including CPU/Memory/Network/IO and connector to other services) ready except your code (computation/function/logic) Lambda as a box In the blue box, you can put any code, say Python, Java, C#, or any binary generated by any language such as Go. Although AWS has an official list of supported languages, with the customized runtime option, you can run binary generated from any languages such as Rust and C++. ","date":"2023-02-04","objectID":"/posts/serverless-2-lambda/:1:0","tags":["serverless","aws","lambda","systematic-trading","stds"],"title":"Systematic Trading Done Serverless (2) - Lambda","uri":"/posts/serverless-2-lambda/"},{"categories":["Software Design"],"content":"Why Using Lambda ","date":"2023-02-04","objectID":"/posts/serverless-2-lambda/:2:0","tags":["serverless","aws","lambda","systematic-trading","stds"],"title":"Systematic Trading Done Serverless (2) - Lambda","uri":"/posts/serverless-2-lambda/"},{"categories":["Software Design"],"content":"Cost Surprisingly, the first reason is cost. You can argue that it seems any server can do what lambda can do! Yes, true. But with lambda you only pay what you used. By used I mean CPU/memory resources. If your lambda functions runs 100ms with 128 MB memory, you pay only that part. whereas in the hosting model, you pay for the host even you don’t run any function. (since you always have to run a operating system for the host to do anything). We know that a lot of computing has a busty nature, ie sometime the computation load is heavy and some time idle. In addition, not all computation require same resources. Having the ability to tailer the resources for different functions can save cost. Systematic Trading Context: Reducing cost is increasing profit, easy. ","date":"2023-02-04","objectID":"/posts/serverless-2-lambda/:2:1","tags":["serverless","aws","lambda","systematic-trading","stds"],"title":"Systematic Trading Done Serverless (2) - Lambda","uri":"/posts/serverless-2-lambda/"},{"categories":["Software Design"],"content":"Auto Scaling Both horizontal and vertical scaling are done easily with lambda. For horizontal scaling, we pretty much do nothing. AWS almost has no cap on how many lambda instances you can run concurrently. For vertical scaling, we just need to specify the memory resource we want. CPU come with memory setup. Although for now AWS has a memory cap at 10 Gb. Systematic Trading Context A lot computation pattern has a busty property. For example, ad-hoc research backtesting, or during some market hours we need to sample more frequently. For stock market, we don’t even trade during holidays. ","date":"2023-02-04","objectID":"/posts/serverless-2-lambda/:2:2","tags":["serverless","aws","lambda","systematic-trading","stds"],"title":"Systematic Trading Done Serverless (2) - Lambda","uri":"/posts/serverless-2-lambda/"},{"categories":["Software Design"],"content":"Language Independent lambda makes writing software in different language so easy. You can build computation intensive function with language like Rust, whereas build routing component with NodeJs. Then you can compose them as they were done in the same language. Although writing application in different languages is a double edged sword, lambda grant developer this ability to do so. As I mentioned before, serverless architecture encourages micro-services and event-driven design. this ability of lambda powers the idea. Systematic Trading Context Tackle bottlenecks with suitable languages ","date":"2023-02-04","objectID":"/posts/serverless-2-lambda/:2:3","tags":["serverless","aws","lambda","systematic-trading","stds"],"title":"Systematic Trading Done Serverless (2) - Lambda","uri":"/posts/serverless-2-lambda/"},{"categories":["Software Design"],"content":"Encourage Event-Driven Design lambda on it’s own cannot do much, we need ways to invoke/trigger our functions. And lambda has lots of event triggers: a api call, a message in the queue, or a scheduler, etc. lambda can interact with pretty much every single aws services. Event Triggers This event-driven nature makes integrating lambda with other services easy. Hence encouraging event-driven design. Event-driven design has a good ability to extend and audit. Systematic Trading Context Systematic trading is a fast evolving business, hence the some parts of the system are constantly changing and audibility is very important as well. ","date":"2023-02-04","objectID":"/posts/serverless-2-lambda/:2:4","tags":["serverless","aws","lambda","systematic-trading","stds"],"title":"Systematic Trading Done Serverless (2) - Lambda","uri":"/posts/serverless-2-lambda/"},{"categories":["Software Design"],"content":"How to Use Lambda In concept: Define resource specification Memory Runtime Environment Variables Permission Define trigger event(s) API Call A message from queue A message from event broker Any event emit from other aws services Upload code or binary In details? We definitely need a separate post. There is another aspect of “how” to use, which is what kind of computation can we use lambda? Web applications Data processing pipelines Parallelized computing Internet of Things (IoT) In this series, we are particularly interested in Data processing amd Parallelized computing. ","date":"2023-02-04","objectID":"/posts/serverless-2-lambda/:3:0","tags":["serverless","aws","lambda","systematic-trading","stds"],"title":"Systematic Trading Done Serverless (2) - Lambda","uri":"/posts/serverless-2-lambda/"},{"categories":["Software Design"],"content":"Why Not Using Lambda With the gain of lambda, there are some drawbacks: Lose of low level control server tuning latency variance control state ( lambda supposed to be stateless ) A lot of async function can make tracking/debugging system hard. Lambda’s 10 gb resource limit is not great ( there is another serverless service call Fargate is designed for larger computation. ) With those in mind, we know that lambda probably not suitable for low latency trading components. And if a computation takes more than 10gb memory and more then 15 min to finish, lambda is not the right tool. The other aspect is that, if you have a computation running 24 hours continuously, lambda is not the right tool. Fargate is the one in serverless toolbox. ","date":"2023-02-04","objectID":"/posts/serverless-2-lambda/:4:0","tags":["serverless","aws","lambda","systematic-trading","stds"],"title":"Systematic Trading Done Serverless (2) - Lambda","uri":"/posts/serverless-2-lambda/"},{"categories":["Software Design"],"content":"Last thought AWS lambda is the fundamental building block of serverless architecture. The benefit of lambda is reducing cost, flexibility, and scaling. In the mean time, there are some drawbacks with lambda such as latency variance, tracing complexity. In the systematic trading context, lambda can be used to do different kind of computations, for example, ad-hoc backtest/simulations, scheduled jobs, sample-based trading systems. Among all the drawbacks, latency variance probably is my biggest concern because some parts of the system do need stable latency ( not necessary low latency ). ","date":"2023-02-04","objectID":"/posts/serverless-2-lambda/:5:0","tags":["serverless","aws","lambda","systematic-trading","stds"],"title":"Systematic Trading Done Serverless (2) - Lambda","uri":"/posts/serverless-2-lambda/"},{"categories":["Software Design"],"content":"References Lambda Operator Guide Serverless Design Principles [SVS311]Practical experience with a serverless-first strategy at Capital One Lambda tuning Best/worst practices AWS Lambda Introduction - What is it and Why is it Useful? ↩︎ ","date":"2023-02-04","objectID":"/posts/serverless-2-lambda/:6:0","tags":["serverless","aws","lambda","systematic-trading","stds"],"title":"Systematic Trading Done Serverless (2) - Lambda","uri":"/posts/serverless-2-lambda/"},{"categories":["Software Design"],"content":"(STDS) Part 1 - Introduction to Serverless.","date":"2023-02-01","objectID":"/posts/serverless-1-intro/","tags":["serverless","aws","systematic-trading","stds"],"title":"Systematic Trading Done Serverless (1) - Introduction","uri":"/posts/serverless-1-intro/"},{"categories":["Software Design"],"content":" This series is about building a systematic trading system prototype using serverless computing. Although the implementation and some explanations are using a specified provider AWS, most of the contents are applied to any serverless providers who provide similar services. I am not sure how many posts are there to complete this series because the prototype is a building in progress as well. Haven’t said that, here is my plan: Introduction to Serverless Computation for short- Lambda Computation for long - Fargate Communication - Queue Communication - Message Bus Storage for objects - S3 Storage for semi-structured data - DynamoDB API design and implementation - API Gateway Orchestration with workflow - Step Functions Serverless systematic trading - Daily Data Serverless systematic trading - Intraday Data Serverless systematic trading - Research Serverless systematic trading - Batched Trading Strategy Serverless systematic trading - Event-driven Trading Strategy Serverless testing - Unit, Integration and Local test Serverless dev-ops - Code Pipeline To Compute or not to compute? Systematic trading is all about computation. whereas Serverless shires when computation is not happening. (I will explain) Let me walk you through how these two seemingly contradictory things work together VERY well. This post expands this way: first, introduce what is serverless and its building blocks. Then we explore why serverless may or may not fit in systematic trading system design and implementation. ","date":"2023-02-01","objectID":"/posts/serverless-1-intro/:0:0","tags":["serverless","aws","systematic-trading","stds"],"title":"Systematic Trading Done Serverless (1) - Introduction","uri":"/posts/serverless-1-intro/"},{"categories":["Software Design"],"content":"What is Serverless? Serverless is the new ( about 8 years old as of 2023 ) cool kid in cloud computing domain. In concept, Serverless gives developer a new set of tools to build application/software. And new tooling often changes the way we design, implement, and deploy our software. Hence the new serverless tooling results a new architecture, namely Serverless Architecture. A discussion of serverless without cloud computing is not completed. Serverless born from Cloud Computing. In cloud computing, comparing to non-cloud computing, the hosting and managing of the infrastructures are off loaded to cloud providers from us ( developers ). However, the tooling is not really changed much, we still think about some linux server/boxes hosted somewhere, and we use container tools to orchestrate our services we coded up. time to time, we need to think about how to scale the servers, how to migrate to a bigger box, how to do load-balancing, etc. To put it simple, with cloud computing, we always have unlimited linux boxes ready to use without thing maintain the hardwares (as long as we pay for it). However, we are still coding against operating systems, our runtime. For example, we need to setup a message broker cluster, say Kafka, to flow events around, to setup a PostgreSQL server, to think about the hard drive spaces on the box, setup JVM on the box to run 2 lines of Java code (sorry, this is a joke, you probably cannot create a 2 line java binary you need 3.), setup Python version to run another python process, etc. Serverless bring us further: it removes the operating system layer from us. Technically speaking, serverless is a set of unlimited services instead of boxes ready to use. In the end, Serverless is not really without server, someone has to do heavy lifting. It is more about thinking without server(s). This leads us to next section: the building blocks. ","date":"2023-02-01","objectID":"/posts/serverless-1-intro/:1:0","tags":["serverless","aws","systematic-trading","stds"],"title":"Systematic Trading Done Serverless (1) - Introduction","uri":"/posts/serverless-1-intro/"},{"categories":["Software Design"],"content":"Serverless Building Blocks What are we thinking when we create a software or an application or even trying to do anything with a computer? Computation Communication Storage For example, if I would like to trade Bitcoin on a 1 min bar data. I may do following: Call some APIs or websocket to stream data (Computation and Communication) Once bar data ticks, I compute my target position based on historical information ( Computation and Communication) To do that, I need to access disk/database to load long history into memory ( Storage ) Once I have target position, orders are computed and submitted to broker ( Computation and Communication ) The process waiting for other events, such as order filles, rejected or next bar ticks As you found may find as well, we are not thinking about where to put the processing in which operating system. We are thinking about three different services: computation, communication, and storage. Please let me use AWS serverless as example to expand each components. ","date":"2023-02-01","objectID":"/posts/serverless-1-intro/:2:0","tags":["serverless","aws","systematic-trading","stds"],"title":"Systematic Trading Done Serverless (1) - Introduction","uri":"/posts/serverless-1-intro/"},{"categories":["Software Design"],"content":"Computation We have 2 tools in computation: AWS Lambda and AWS Fargate. Lambda is an event-driven, pay-as-you-go compute service that let you run code without provisioning or managing servers. You can think Lambda as a ready to use container with several runtime time already setup for you. There are only two things we plug into Lambda: our business logic/computation code and resources and runtime we want. Out of box, Lambda support mainstream runtimes such as python, nodejs, go, rust, Java, C#. Lambda is suitable for short lived (less than 15 minutes) and small (less than 10gb memory) computation. What about long lived and big computation that Lambda cannot handle? We have Fargate. Fargate is basically a container runtime managed by aws. There are also two things we plugin: our code image and resource limit. In short, serverless computation is about runtime + code and resource we need. ","date":"2023-02-01","objectID":"/posts/serverless-1-intro/:2:1","tags":["serverless","aws","systematic-trading","stds"],"title":"Systematic Trading Done Serverless (1) - Introduction","uri":"/posts/serverless-1-intro/"},{"categories":["Software Design"],"content":"Storage I view storage as three levels: long term with limited query ability, like a file system long term with reach query ability, like a database short cache, like Redis AWS gives us more than those categories, but let us focus on those: S3: long term object store, where we can put anything, text, csv, binary DynamoDB: long term semi-structured store, where we put data that we want to query against ElasticCache: low latency cache. ( AWS claims it has microsecond level latency.. ) All above storage service we can assume: no storage limit no concurrency limit auto scaling Sounds good right? Yeah, but we need to pay the bill. ","date":"2023-02-01","objectID":"/posts/serverless-1-intro/:2:2","tags":["serverless","aws","systematic-trading","stds"],"title":"Systematic Trading Done Serverless (1) - Introduction","uri":"/posts/serverless-1-intro/"},{"categories":["Software Design"],"content":"Communication and Integration Communication services are the key to hook everything together. We have to patterns of communication: Queue based AWS SQS Message Bus based ( Pub/Sub ) AWS SNS AWS EventBridge API pulling AWS API Gateway Workflow AWS Step Function SQS is a fully managed message queuing for microservices, distributed systems, and serverless applications. SNS is fully managed Pub/Sub service for A2A and A2P messaging. EventBridge is build event-driven applications at scale across AWS, existing systems, or SaaS applications API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. Step Function is a new service which is a visual workflow service that helps developers use AWS services to build distributed applications, automate processes, orchestrate microservices, and create data and machine learning (ML) pipelines. ","date":"2023-02-01","objectID":"/posts/serverless-1-intro/:2:3","tags":["serverless","aws","systematic-trading","stds"],"title":"Systematic Trading Done Serverless (1) - Introduction","uri":"/posts/serverless-1-intro/"},{"categories":["Software Design"],"content":"Summary Those three types of services are exactly our new serverless building blocks. And they are always ready for us to use, to scale up and down. Put it in this way, there will always unlimited CPU and memory, unlimited scaled message broker, and unlimited storage readly for us to deploy our logical code and integration logic. We can build pretty much anything with those components, of course including a systematic trading system. This leads us to the next section: systematic trading software. But before that, let’s briefly discussion the serverless architecture. ","date":"2023-02-01","objectID":"/posts/serverless-1-intro/:2:4","tags":["serverless","aws","systematic-trading","stds"],"title":"Systematic Trading Done Serverless (1) - Introduction","uri":"/posts/serverless-1-intro/"},{"categories":["Software Design"],"content":"Serverless Architecture Architecture is too big as topic for a section in a blog. But I would like mention a few thing that I found interesting when it comes to serverless architecture. The tools we use have a profound (and devious) influence on our thinking habits, and, therefore, on our thinking abilities. By Edsger W. Dijkstra Serverless dynamically changed our tool box, hence changed the way we design, implement, and deploy our software. One of the biggest change is that serverless encourages Event-driven design and fearless scaling. The other thing is that we need to think more about the cost of running the software. Since almost all the services we have is pay as you go model, our cost is not constant any more, like the good old days we have some boxes running for us all day long even doing nothing. Because the cost is dynamic now, we need to think about how to optimize it. (this is why I was saying serverless shires when computation not happening. We only pay when when compute.) The last thing is scale of the project. Serverless Architecture is really fantastic in the sense that it fits from very small ad-hoc project to large enterprise projects. And as soon I adapt serverless, the design for smaller project can move to large scale stepup incrementally. For a startup type product without much traffic, aws’s free tier plan will give us nearly free ride of the whole infrastructure. When the project get more attention, the same code scales automatically. This is simply amazing. ","date":"2023-02-01","objectID":"/posts/serverless-1-intro/:3:0","tags":["serverless","aws","systematic-trading","stds"],"title":"Systematic Trading Done Serverless (1) - Introduction","uri":"/posts/serverless-1-intro/"},{"categories":["Software Design"],"content":"Systematic Trading Software Let’s analyses what properties a systematic trading system should have: [p1] handle the different velocities of data, from ms to days or even months. [p2] handle different resource limitations. The resources here are CPU cores, memory capacity, disk space, and most importantly the time to finish the computation. [p3] handle ever-increasing new predictors, new datasets, and new models that are added into the system. [p4] system should be available all the time ( especially in the crypto world, market opens 24 * 7) [p5] reduce costs of all kinds. This includes capital to buy and maintain the infrastructure, and human resources to manage the infrastructure. [p6] be reactive. The system should be reactive to different events such as order filled, market data, or human intervention. [p7] be flexible. Handle new forms of data, trading rules, etc. [p8] time to market should be as short as possible. [p9] some systems need low latency. For example, the time between a market event to order execution can be limited to sub-milliseconds. [p10] can be debugged and fixed quickly. If the systems show abnormal behavior, we should be able to identify and fix the problem quickly. ","date":"2023-02-01","objectID":"/posts/serverless-1-intro/:4:0","tags":["serverless","aws","systematic-trading","stds"],"title":"Systematic Trading Done Serverless (1) - Introduction","uri":"/posts/serverless-1-intro/"},{"categories":["Software Design"],"content":"Serverless: the Good Scalability High Availability Reducing Cost Flexibility Low Operation Overhead ","date":"2023-02-01","objectID":"/posts/serverless-1-intro/:5:0","tags":["serverless","aws","systematic-trading","stds"],"title":"Systematic Trading Done Serverless (1) - Introduction","uri":"/posts/serverless-1-intro/"},{"categories":["Software Design"],"content":"Scalability Serverless computing automatically scales resources based on demand, allowing applications to handle sudden spikes in traffic without any additional effort or cost. This fits p1, p2, and p3: p1: The ability to scale up and down based on which part of the system is running is a huge win in terms of infrastructure admin. p2: The ability to provide different resources given the limitations on the fly is very beneficial. p3: serverless services normally can scale without any overhead. ","date":"2023-02-01","objectID":"/posts/serverless-1-intro/:5:1","tags":["serverless","aws","systematic-trading","stds"],"title":"Systematic Trading Done Serverless (1) - Introduction","uri":"/posts/serverless-1-intro/"},{"categories":["Software Design"],"content":"High availability Most serverless providers offer built-in high availability, which helps ensure that your applications are available even in the case of infrastructure failures. This maps to p4. ","date":"2023-02-01","objectID":"/posts/serverless-1-intro/:5:2","tags":["serverless","aws","systematic-trading","stds"],"title":"Systematic Trading Done Serverless (1) - Introduction","uri":"/posts/serverless-1-intro/"},{"categories":["Software Design"],"content":"Reducing cost This fits p5. Cost is important in trading in general, as cost in trading may turn a profitable trade into a loss. However, since we invest so much in the infrastructure to support our trading system, the cost of the infrastructure and operation becomes increasingly large. With serverless, you only pay for the specific resources you use, rather than paying for a fixed amount of resources upfront. This can lead to significant cost savings, especially for applications with variable or unpredictable workloads. Although it is difficult to know the details of the design of different companies (it is still a bit secret industry even in an open Internet age ). However, we do know companies spend a huge amount of money and resource to build their in-house infrastructure to support their systematic trading business. Some companies are adapting to outsource their infrastructure to cloud providers, ie moving to cloud computing. Serverless is a natural fit, as serverless is by nature cloud computing at its core. Any performance optimizations you make to your code will not only increase the speed of your app, but they’ll have a direct and immediate link to a reduction in operational costs, ","date":"2023-02-01","objectID":"/posts/serverless-1-intro/:5:3","tags":["serverless","aws","systematic-trading","stds"],"title":"Systematic Trading Done Serverless (1) - Introduction","uri":"/posts/serverless-1-intro/"},{"categories":["Software Design"],"content":"Flexibility Serverless architecture forces to decouple the system component due to the way it manages the computation resources, leveraging microservices and message-passing patterns. Another aspect is that most of the serverless providers, Amazon AWS, provide a different runtime environment for a different types of languages. This means we could introduce different languages in different components. For example, we could implement an order management system by compiled languages such as Rust and Java; and we could realise a predictor component by Python leveraging the powerful data analysis ecosystem of Python. This maps to p7. ","date":"2023-02-01","objectID":"/posts/serverless-1-intro/:5:4","tags":["serverless","aws","systematic-trading","stds"],"title":"Systematic Trading Done Serverless (1) - Introduction","uri":"/posts/serverless-1-intro/"},{"categories":["Software Design"],"content":"Reducing operation overhead and better integration Serverless providers offer a wide range of services that can be easily integrated with your applications, such as databases, message queues, and storage. Serverless allows you to build event-driven applications, which can automatically trigger functions in response to specific events, such as changes to a database or the arrival of new data. This fits in p6, p7, and p8. ","date":"2023-02-01","objectID":"/posts/serverless-1-intro/:5:5","tags":["serverless","aws","systematic-trading","stds"],"title":"Systematic Trading Done Serverless (1) - Introduction","uri":"/posts/serverless-1-intro/"},{"categories":["Software Design"],"content":"Serverless: the Bad Cold Start Unstable Latency Vendor Lock-in Hard to Debug Limited State Management Complex Choreography Noticed that p9 and p10 are not mentioned in the previous section. ","date":"2023-02-01","objectID":"/posts/serverless-1-intro/:6:0","tags":["serverless","aws","systematic-trading","stds"],"title":"Systematic Trading Done Serverless (1) - Introduction","uri":"/posts/serverless-1-intro/"},{"categories":["Software Design"],"content":"Cold start This may be harmful to p9. ","date":"2023-02-01","objectID":"/posts/serverless-1-intro/:6:1","tags":["serverless","aws","systematic-trading","stds"],"title":"Systematic Trading Done Serverless (1) - Introduction","uri":"/posts/serverless-1-intro/"},{"categories":["Software Design"],"content":"Unstable Latency This may be harmful to p9. ","date":"2023-02-01","objectID":"/posts/serverless-1-intro/:6:2","tags":["serverless","aws","systematic-trading","stds"],"title":"Systematic Trading Done Serverless (1) - Introduction","uri":"/posts/serverless-1-intro/"},{"categories":["Software Design"],"content":"Vendor lock-in This may increase the cost of switching vendors, hence p5. ","date":"2023-02-01","objectID":"/posts/serverless-1-intro/:6:3","tags":["serverless","aws","systematic-trading","stds"],"title":"Systematic Trading Done Serverless (1) - Introduction","uri":"/posts/serverless-1-intro/"},{"categories":["Software Design"],"content":"Limited debug and monitoring options This may be harmful to p10. ","date":"2023-02-01","objectID":"/posts/serverless-1-intro/:6:4","tags":["serverless","aws","systematic-trading","stds"],"title":"Systematic Trading Done Serverless (1) - Introduction","uri":"/posts/serverless-1-intro/"},{"categories":["Software Design"],"content":"Limited on state management and duration This may be harmful to p7. ","date":"2023-02-01","objectID":"/posts/serverless-1-intro/:6:5","tags":["serverless","aws","systematic-trading","stds"],"title":"Systematic Trading Done Serverless (1) - Introduction","uri":"/posts/serverless-1-intro/"},{"categories":["Software Design"],"content":"Complex choreography This may be harmful to p8. ","date":"2023-02-01","objectID":"/posts/serverless-1-intro/:6:6","tags":["serverless","aws","systematic-trading","stds"],"title":"Systematic Trading Done Serverless (1) - Introduction","uri":"/posts/serverless-1-intro/"},{"categories":["Software Design"],"content":"The End In this series, we will explore different serverless patterns that can be used to build robust systematic trading/research system, and how can we reduce the downside of serverless within systematic trading software. One thing I would like to mention, the native serverless tool set is NOT suitable for trading system requires low latency. By low latency, I mean any latency under 100ms. The reason is that all the messaging services (SNS, SQS, EventBridge) has a median latency ranging from 16ms to 400ms, and it’s p99 latency can reach 1500 ms for step function!1 Haven said that, it does not mean serverless cannot help with low latency trading. We all know that even for a low latency trading system, not all the parts requires the same latency limit. The strong benefit for serverless is that you can migrate part of the system and integrate it into original software easily and incrementally. All right, I hope this part makes you feel excited to build trading system with serverless. With these new cool tools in the box, let’s start the journey. ","date":"2023-02-01","objectID":"/posts/serverless-1-intro/:7:0","tags":["serverless","aws","systematic-trading","stds"],"title":"Systematic Trading Done Serverless (1) - Introduction","uri":"/posts/serverless-1-intro/"},{"categories":["Software Design"],"content":"Additional Fun Facts This figure gives you an idea the scale of serverless usage these days2: AWS lambda usage There are 10 trillion lambda invocations (the computation service of serverless)! https://bitesizedserverless.com/bite/serverless-messaging-latency-compared/ ↩︎ https://www.youtube.com/watch?v=0_jfH6qijVY ↩︎ ","date":"2023-02-01","objectID":"/posts/serverless-1-intro/:8:0","tags":["serverless","aws","systematic-trading","stds"],"title":"Systematic Trading Done Serverless (1) - Introduction","uri":"/posts/serverless-1-intro/"}]