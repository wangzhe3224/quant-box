[{"categories":["Data Engineering"],"content":"How to schedule a computation graph (DAG) on AWS Step Functions, ie serverless","date":"2023-03-07","objectID":"/posts/serverless-9-step-functions/","tags":["aws","step-functions","stds","dag"],"title":"AWS Step Functions and DAG","uri":"/posts/serverless-9-step-functions/"},{"categories":["Data Engineering"],"content":"Oh DAG! Direct acyclic graph, DAG, is a popular data structure to organize large scale computation. because it allows you to represent the dependencies between different tasks in a clear and efficient manner. DAGs are commonly used in data processing, scientific computing, and machine learning applications to manage complex workflows. Here are some key reasons why DAGs are useful for managing computation: Clear representation of dependencies: A DAG clearly shows the dependencies between different tasks, making it easy to understand which tasks must be completed before others can begin. Efficient scheduling: With a DAG, it’s easy to schedule tasks in the most efficient way possible. By prioritizing tasks based on their dependencies, you can ensure that each task is executed only when its input data is available, which minimizes unnecessary waiting times. Parallelism: DAGs can also be used to identify opportunities for parallelism, which can significantly speed up computation. Tasks that don’t have dependencies can be executed in parallel, making better use of available resources. Incremental computation: DAGs can be used to efficiently perform incremental computations, where only the necessary parts of a computation are re-run when new data becomes available. This can save a significant amount of time and resources compared to re-running the entire computation from scratch. Overall, DAGs are a powerful tool for managing complex computations, enabling efficient scheduling, parallelism, and incremental computation, all while providing a clear representation of dependencies between tasks. As we can see, with DAG we can significantly parallelize our computations and the power of DAG let us to do fearless concurrent computation which fits in the serverless land very well. ","date":"2023-03-07","objectID":"/posts/serverless-9-step-functions/:1:0","tags":["aws","step-functions","stds","dag"],"title":"AWS Step Functions and DAG","uri":"/posts/serverless-9-step-functions/"},{"categories":["Data Engineering"],"content":"Existing DAG Based Engines There are several libraries and frameworks that use DAGs for managing computation. Here are some examples: Apache Airflow: Apache Airflow is a popular open-source platform for managing workflows and data pipelines. It uses DAGs to represent the dependencies between tasks, and provides a rich set of tools for scheduling, monitoring, and executing tasks. Dask: Dask is a Python library for parallel computing that uses DAGs to represent computation graphs. It provides a flexible framework for executing parallel and distributed computations on large datasets. TensorFlow: TensorFlow is a popular machine learning library that uses DAGs to represent computation graphs. It allows users to define complex machine learning models as a series of interconnected nodes, with each node representing a mathematical operation. Luigi: Luigi is an open-source Python package for building data pipelines. It uses DAGs to represent the dependencies between tasks, and provides a flexible framework for scheduling and executing tasks. Spark: Apache Spark is a popular distributed computing framework that uses DAGs to represent computations. It provides a powerful set of tools for parallel and distributed processing of large datasets. These libraries and frameworks are just a few examples of the many tools that use DAGs to manage computation. By using DAGs, these tools can provide a clear representation of dependencies between tasks, efficient scheduling, and parallelism, which can make it easier to manage complex workflows and large datasets. There are also some libraries, such as Incremental (JaneStreet), Man MDF (Man Group), loman, anchors, etc. More details please check: https://wangzhe3224.github.io/awesome-systematic-trading/#computation-graph ","date":"2023-03-07","objectID":"/posts/serverless-9-step-functions/:2:0","tags":["aws","step-functions","stds","dag"],"title":"AWS Step Functions and DAG","uri":"/posts/serverless-9-step-functions/"},{"categories":["Data Engineering"],"content":"Schedule The DAG Apart from describe the computation, another important aspect of using DAG for computation is the runtime or scheduling runtime. There are different ways to schedule DAG computation based on the nature of the tasks, available resources, and performance goals. Here are some examples of scheduling strategies: Serial scheduling: In this approach, tasks are executed in a sequential manner, one after the other. This is the simplest scheduling strategy and may be appropriate for small DAGs with a small number of tasks and dependencies. Parallel scheduling: Tasks are executed concurrently, taking advantage of multiple processors or threads. Parallelism can be used when tasks are independent or can be executed in parallel without interfering with each other. This can speed up computation significantly and is a common strategy for large-scale computations. Topological ordering: A topological ordering of the DAG is a linear ordering of the nodes such that if there is an edge from node A to node B, then A comes before B in the ordering. This ordering can be used to schedule tasks, with tasks at the beginning of the ordering executed first, followed by tasks later in the ordering. Dynamic scheduling: In this approach, tasks are scheduled at runtime based on the availability of data and resources. This can be useful when the execution time of tasks is uncertain or when new tasks are added to the DAG at runtime. Batch scheduling: In this approach, tasks are grouped into batches, and each batch is executed in parallel. Batching can be useful when the DAG contains a large number of tasks and parallelism can be used to speed up computation. Pipeline scheduling: In this approach, tasks are divided into stages, and each stage is executed in parallel. Each stage can have its own set of dependencies and resources, allowing for efficient use of available resources. This approach is commonly used in data processing and machine learning applications. These scheduling strategies can be combined in different ways to optimize performance and resource usage for a given DAG. The choice of scheduling strategy depends on the characteristics of the DAG and the available resources. ","date":"2023-03-07","objectID":"/posts/serverless-9-step-functions/:3:0","tags":["aws","step-functions","stds","dag"],"title":"AWS Step Functions and DAG","uri":"/posts/serverless-9-step-functions/"},{"categories":["Data Engineering"],"content":"Can DAG Go Serverless? Yes. As we discussed, there are two parts in computation with DAG: computing and scheduling. Computing is the bread and butter of serverless, we have Lambda or Fargate. However, scheduling is not that straightforward. As shown above, most of the DAG based computation engine, such as Spark, we need a long running session to coordinate the computation. This long run session need to handle the scheduling of the jobs and handling the intermediate results. Serverless is not very good at long running tasks. Luckily enough, AWS provides a workflow service: Step Functions. Step Functions is not a service designed for DAG computation, however we can make it our DAG scheduling engine with a bit efforts. ","date":"2023-03-07","objectID":"/posts/serverless-9-step-functions/:4:0","tags":["aws","step-functions","stds","dag"],"title":"AWS Step Functions and DAG","uri":"/posts/serverless-9-step-functions/"},{"categories":["Data Engineering"],"content":"Topological Order Say for example, I have following DAG: First of all, we get the topological order of the DAG as show above. With this order, the jobs are organized to several layers, for example, root node is at layer 0, xyz are in layer 1, etc. ","date":"2023-03-07","objectID":"/posts/serverless-9-step-functions/:4:1","tags":["aws","step-functions","stds","dag"],"title":"AWS Step Functions and DAG","uri":"/posts/serverless-9-step-functions/"},{"categories":["Data Engineering"],"content":"Transitive Reduction Notice that there is a dependency from x to x+(x+z) node, which is a dependency goes across two layers. This is not good! Because Step Function do not support this kind of workflow. However, if we look at it closely, this dependency is redundant in this setup, because by the time we reach x+(x+z), x is guaranteed fulfilled. This is called transitive reduction, meaning G = (V,E) is a graph G- = (V,E-) such that for all v,w in V there is an edge (v,w) in E- if and only if (v,w) is in E and there is no path from v to w in G with length greater than 1. So we can reduce the graph: This is nice and clean now and we can use Step Function to express this DAG now. ","date":"2023-03-07","objectID":"/posts/serverless-9-step-functions/:4:2","tags":["aws","step-functions","stds","dag"],"title":"AWS Step Functions and DAG","uri":"/posts/serverless-9-step-functions/"},{"categories":["Data Engineering"],"content":"Pass Parameters Transitive reduction is good, however there are some node missing input information. For example, x+(x+z) can never know it need to read x. Using Step Functions to schedule DAG is a static scheduling, meaning we know exactly the execution order, so we can encode the input and output into the config statically before running the DAG, see input and output of AWS Step Function for details.1 ","date":"2023-03-07","objectID":"/posts/serverless-9-step-functions/:4:3","tags":["aws","step-functions","stds","dag"],"title":"AWS Step Functions and DAG","uri":"/posts/serverless-9-step-functions/"},{"categories":["Data Engineering"],"content":"Intermediate Results The next piece is how can we manage the intermediate results? There could be many serverless solutions. For example, we can use S3. If we care more about the latency, we can use ElasticCache. ","date":"2023-03-07","objectID":"/posts/serverless-9-step-functions/:4:4","tags":["aws","step-functions","stds","dag"],"title":"AWS Step Functions and DAG","uri":"/posts/serverless-9-step-functions/"},{"categories":["Data Engineering"],"content":"General Lambda The last piece is what’s actually in the Lambda Invoke job in the Step Function workflow? This function should know how to load parameter from the intermediate store (previous step), then should also know which function to call to compute the result, and at last store the output to the store. Here is some demo code: store = S3Store(bucket=\"some-bucket\") path = \"store/\" def handler(event, context): \"\"\" Event example: { \"component_name\": \"HistDataLoader\", \"component_config\": { \"symbol\": \"ETHUSDT\", \"freq\": \"5m\", \"lookback\": 1, \"type\": \"future\" }, \"args\": null, \"kwargs\": null } \"\"\" component_name, config, args, output = event['component_name'], event['component_config'], event['args'], event['output'] _cls = getattr(components, component_name) obj = _cls(**config) if args is None: res = obj() else: argv = [] for arg in args: d = store.read(f\"{path}{arg}\") argv.append(d) res = obj(*argv) store.write(f\"{path}{output}\", res) return { \"output\": f\"{path}{output}\" } ","date":"2023-03-07","objectID":"/posts/serverless-9-step-functions/:4:5","tags":["aws","step-functions","stds","dag"],"title":"AWS Step Functions and DAG","uri":"/posts/serverless-9-step-functions/"},{"categories":["Data Engineering"],"content":"Potential Issues Some times, how to arrange the jobs in different layer is not straightforward. Take following graph for example: node 2x only depends on x, however, if we follow the standard compilation method, 2x can only be executed after all xyz nodes finished. Another way to schedule 2x is to put it into the same layer of xyz, so that 2x executed immediately after x is done. However, in the way, we are risking slow down the computation of the whole next layer. For example, if 2x is a very expensive computation in terms of time, it is better to schedule it in the next layer. Take it further, event 2x is scheduled in the next layer, the long computation delay problem is not resolved but just pus the delay a bit further. Fundamentally, if we cannot predict the runtime of each job, workflow approach will suffer this type of issue. In order to solve this potential issue, a dynamic scheduling is required, whereas essentially workflow is a static scheduling. I will leave that with another post. https://docs.aws.amazon.com/step-functions/latest/dg/concepts-input-output-filtering.html ↩︎ ","date":"2023-03-07","objectID":"/posts/serverless-9-step-functions/:5:0","tags":["aws","step-functions","stds","dag"],"title":"AWS Step Functions and DAG","uri":"/posts/serverless-9-step-functions/"},{"categories":["Data Engineering"],"content":"Demo how to build a data processing pipeline with serverless on aws.","date":"2023-02-17","objectID":"/posts/insider_trade_pipeline/","tags":["aws","serverless","alternative-data","stds"],"title":"Serverless Data Pipeline - SEC Insider Trades (Form 4)","uri":"/posts/insider_trade_pipeline/"},{"categories":["Data Engineering"],"content":"Introduction In this post we will demo the design of a serverless data pipeline with aws using SEC Insider Trade (Form 4) data as an example, which is public available. This post is part of the implementation of Systematic Trading Done Serverless Series. Securities and Exchange Commission, SEC, is a government agency mainly to enforce law against security market manipulation1. With Form 4 filing, the public is made aware of the insider’s various transactions in company securities, including the amount purchased or sold and the price per share. An “insider” is an officer, director, 10% stockholder and anyone who possesses inside information because of his or her relationship with the Company or with an officer, director or principal stockholder of the Company. Form 4 must be filed within two business days following the transaction date. More details check here. The form is delivered as XML file, for example a form 4 for AAPL and here is the same from in HTML format. Screenshot as below2: Basically, Form 4 discloses insider’s transactions of the company stocks or derivatives. The information we care is that: Who traded, i.e. is he/she a director or officer or 10% owner? How many shares at what price? When traded, When reported? Transaction type: directly trade? Option exercise? Buy or Sell? The goal is: give a universe of companies (identified by SEC id CIK), check if there are any new Form 4 filings uploaded, if so download the XML files and extract the insider trade information, then persist them for later usage. It would also be good that we can get some form of notification if new insider trade happened. ","date":"2023-02-17","objectID":"/posts/insider_trade_pipeline/:1:0","tags":["aws","serverless","alternative-data","stds"],"title":"Serverless Data Pipeline - SEC Insider Trades (Form 4)","uri":"/posts/insider_trade_pipeline/"},{"categories":["Data Engineering"],"content":"Design Decisions Here we break the design decision to following parts: Decision 1: How will the system scale up? Decision 2: Pick up tools from each categories of the services Decision 3: Use queue and lambda to achieve parallelization Decision 4: What kind of storage should we use to store information Decision 5: Design error path as normal path, let exception throw loudly Here is a diagram of the design in high level: High Level Design Let’s break it down. ","date":"2023-02-17","objectID":"/posts/insider_trade_pipeline/:2:0","tags":["aws","serverless","alternative-data","stds"],"title":"Serverless Data Pipeline - SEC Insider Trades (Form 4)","uri":"/posts/insider_trade_pipeline/"},{"categories":["Data Engineering"],"content":"How the Pipeline Scales Decision 1: Following id based parallelization patterns. In systematic trading world, nearly every predictor/signal starts with a data pipeline. Most of the data pipelines are name by name basis, meaning given a company or entity, there are multiple fields related to it. For example, Apple Inc., we have price data, order book data, fundamental data, alternative data such as social media, search activity, event data. As we can see there are two dimensions about the data: identifier and field. It is not entirely true, we also have a time dimension! id, N field, F time, T The next question to ask is: in which dimension we process the data? The objective function is speed, ie how can we process the data in the fastest way? I think the answer drills down to how well we parallelize our data pipeline. We can parallelize by id, by field, or event by time! Which one should we choose? It depends on which one is larger and more uniform. Time is not a good choice in my opinion as time is not even for all the ids and fields. And most of the time, the number of ids, N, is much larger than the number of fields, F. In this case, it is better to build the data pipeline by id basis, so that we gain the most ability to parallelize. In the end, the decision is we use company names as our parallelization path. ","date":"2023-02-17","objectID":"/posts/insider_trade_pipeline/:2:1","tags":["aws","serverless","alternative-data","stds"],"title":"Serverless Data Pipeline - SEC Insider Trades (Form 4)","uri":"/posts/insider_trade_pipeline/"},{"categories":["Data Engineering"],"content":"Serverless Data Pipeline Decision 2: Pick up tools from each categories of the services In the previous posts, we went through quite a few serverless tools. How to build data pipeline with to serverless tools? As I proposed before, in serverless land, we think in three components, computation, communication, adn storage: Computation Lambda, for io requests, parsing the file and extracting information Communication EventBridge, for scheduling SQS, for task management Storage S3, for file storage DynamoDB, for post-processed data query ","date":"2023-02-17","objectID":"/posts/insider_trade_pipeline/:2:2","tags":["aws","serverless","alternative-data","stds"],"title":"Serverless Data Pipeline - SEC Insider Trades (Form 4)","uri":"/posts/insider_trade_pipeline/"},{"categories":["Data Engineering"],"content":"Parallelization Decision 3: Use queue and lambda to achieve parallelization Since companies filings are independent, we can parallelize the pipeline very well using lambda and queue. This is a common design pattern in serverless land, namely queue-lambda. The idea is the have a daily scheduled job kick off to enqueue all company names, ie the universe we care. After all the jobs are in the queue, we define a lambda function to check and download filing files if any. The benefit of lambda is that we can nearly parallelize every names. The benefit of a queue-lambda structure is that we can control our parallelization via lambda’s reserved concurrency. For example, if we set the reserved concurrency to 10, we will not processing more than 10 names at the same time. There is another subtile yet interesting benefit of lambda: every code start lambda instance has a different public IP address! This is really a great feature in terms scaping type data pipeline. As we probably all know that most of the public available data source have some sort of throttling control. Say for example the SEC website has a rate limit of 10 requests per second. With the different lambda instance with different IPs, we actually increase the limit! (please… don’t do anything bad with this ….) I did some experiments to prove this, here is the calling history of a lambda function. The function is easy simply print its IP address: @2023-02-01 20:00 {\"ip\": \"18.170.55.26\"} @2023-02-01 23:30 {\"ip\": \"18.169.190.4\"} @2023-02-01 23:48 {\"ip\": \"18.134.99.111\"} @2023-02-02 11:17 {\"ip\": \"3.8.162.74\"} @2023-02-02 20:39 {\"ip\": \"18.168.205.156\"} @2023-02-02 22:04 {\"ip\": \"13.42.48.110\"} It turns out, we can get quite a few of public IP address for free with lambda. ","date":"2023-02-17","objectID":"/posts/insider_trade_pipeline/:2:3","tags":["aws","serverless","alternative-data","stds"],"title":"Serverless Data Pipeline - SEC Insider Trades (Form 4)","uri":"/posts/insider_trade_pipeline/"},{"categories":["Data Engineering"],"content":"SQL or NoSQL or S3 Decision 4: What kind of storage should we use to store information In this pipeline, there are two pieces of information I would like to keep: the raw Form 4 and processed Form 4. Raw data is just the file we download from SEC website. The processed data is bit more interesting. I only picked following fields: res = { \"owner\": owner, \"cik\": cik, \"name\": name, \"symbol\": symbol, \"tx_date\": tx_date, \"tx_code\": tx_code, \"tx_side\": tx_side, \"tx_shares\": tx_shares, \"post_tx_amount\": post_tx_amount, \"access_number\": access_number, \"filing_date\": filing_date, } As long as we persist the raw data files, it is always possible to add another downstream jobs to produced another set of post-process information. Next question is: where shall we store the data? Raw Data For the raw data, it is obvious that S3 is the place to go! The interesting part is how can we design the folder and file name. I proposed following: {BUCKET}/{CIK}/{From}/{FilingDate}#{AccessNumber}.txt. Although this pipeline only focus on Form 4, I still add a sub path {From} to identify potential more filing forms in the future. Processed Data Processed data is almost like a dict in Python, it is highly structured (more structural then XML? Not sure.). And it is a timeseries data by nature. We have few options here: S3, DynamoDB (No-SQL), and SQL. S3 seems not very good as lack of query abilities out of box. We could use is as a underlying file system and build a query layer on top, but it sounds to much and we have better options. I indeed hesitate about DynamoBD and SQL (Serverless version AWS RDS - A relational database). For DynamoDB is THE goto serverless database on aws, it is highly scalable and easy to setup. The tricky bit is the primary key and secondary index design, see details. On the other hand, RDS a SQL database can never go wrong, right?? Tables, Primary key, foreign key, on top all of these, we have powerful query planner, we can do all sort of join, aggregation on the fly. Let’s have a look at our data again. It is simple, a key value pair. Let’s say we have 2000 names, 1 files per day. The amount of the data is still very small. Both DynamoDB and RDS fit our purpose. How to select? The answer is a bit surprise: cost. With our data, DynamoDB is FREE to use. So the decision is DynamoDB! ","date":"2023-02-17","objectID":"/posts/insider_trade_pipeline/:2:4","tags":["aws","serverless","alternative-data","stds"],"title":"Serverless Data Pipeline - SEC Insider Trades (Form 4)","uri":"/posts/insider_trade_pipeline/"},{"categories":["Data Engineering"],"content":"Error Handling Decision 5: Design error path as normal path, let exception throw loudly In our data pipeline there are two potential failure points: Filing download job Filing processing job The download job is a web io job, it could fail for all sort of reasons, limit cap, network error, etc. The processing job could fail as well, for example, a change XML structure could just fail our parser. We have to things to handle failures: retry and dead-letter queue. We could config lambda function to retry 2 time, after all of them failed, the job will be pushed into a dead-letter queue, which will trigger downstream notification to alert the failures. ","date":"2023-02-17","objectID":"/posts/insider_trade_pipeline/:2:5","tags":["aws","serverless","alternative-data","stds"],"title":"Serverless Data Pipeline - SEC Insider Trades (Form 4)","uri":"/posts/insider_trade_pipeline/"},{"categories":["Data Engineering"],"content":"Notification Notification on aws is SNS. There are two types of notifications: on purpose and on failures. In this design, we use DynamoDB stream to publish topics to SNS for on purpose notification. Whereas, on failures notification is done with a lambda consume dead-letter queue items, then publish to a failure topic in SNS. ","date":"2023-02-17","objectID":"/posts/insider_trade_pipeline/:2:6","tags":["aws","serverless","alternative-data","stds"],"title":"Serverless Data Pipeline - SEC Insider Trades (Form 4)","uri":"/posts/insider_trade_pipeline/"},{"categories":["Data Engineering"],"content":"Implementation Ah wow, it takes some time to reach this point: implementation! Given this post becomes too long to read (I am trying to limit each post under 2000 words), I will leave the details with next post. Some key points to encourage you keep an eye: How to develop serverless data pipeline with AWS SAM? How to design the dynamoDB primary key for processed data? How to set batch behavior for queue and lambda? How S3 can trigger a lambda worker? How to handle reentry of the lambda function? How to avoid duplicated jobs? (as we know standard SQS is at least once delivery not exact once delivery) I am also working to release the source code for the implementation as well. ","date":"2023-02-17","objectID":"/posts/insider_trade_pipeline/:3:0","tags":["aws","serverless","alternative-data","stds"],"title":"Serverless Data Pipeline - SEC Insider Trades (Form 4)","uri":"/posts/insider_trade_pipeline/"},{"categories":["Data Engineering"],"content":"Summary In this post, we went through the design process of a serverless data pipeline. The example dataset is SEC Insider Trader filing form 4. We discussed varies design decisions, such as scalability, parallelization, storage, and error handling. Interestingly, some time cost is also a design perspective in serverless. For example, for this design, if we run about 505 names, it is almost free! After the SNS topic of filing topic update, we could develop other downstream jobs, for example computing a signal based on insider trade event. Or some aggregation job to transform the post-processed results to other easy to use format. https://www.investopedia.com/terms/s/sec.asp ↩︎ https://www.sec.gov/edgar/browse/?CIK=320193\u0026owner=exclude ↩︎ ","date":"2023-02-17","objectID":"/posts/insider_trade_pipeline/:4:0","tags":["aws","serverless","alternative-data","stds"],"title":"Serverless Data Pipeline - SEC Insider Trades (Form 4)","uri":"/posts/insider_trade_pipeline/"}]